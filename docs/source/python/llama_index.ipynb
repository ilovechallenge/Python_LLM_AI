{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b3ceb9d-f4cd-4fcc-8407-70d476e779c4",
   "metadata": {},
   "source": [
    "# LlamaIndex Integration ðŸ¦™"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a310c2-ef47-45ff-886f-233dc482e90d",
   "metadata": {},
   "source": [
    "LMQL can be used with the [LlamaIndex](https://github.com/jerryjliu/llama_index) python library. This notebook shows how you can query a LlamaIndex data structure as part of an LMQL query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c315325-4b72-48f2-8416-71c2f36e24aa",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40854abd-55fb-418c-bbdf-b587577ae84f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index import GPTSimpleVectorIndex, SimpleDirectoryReader, ServiceContext\n",
    "import lmql"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe4fe3a-5bdf-43fd-b6f3-19db6df015f2",
   "metadata": {},
   "source": [
    "### Load Documents and Build Index\n",
    "\n",
    "Load documents using SimpleDirectoryReader, and build a GPTSimpleVectorIndex (an index that uses an in-memory embedding store)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bd91a6-179d-4b60-ae8c-23eaafa14210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load documents\n",
    "documents = SimpleDirectoryReader('../../paul_graham_essay/data').load_data()\n",
    "\n",
    "service_context = ServiceContext.from_defaults(chunk_size_limit=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ea0a21-e018-471d-9395-76cb39b86404",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = GPTSimpleVectorIndex.from_documents(documents, service_context=service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c199a3-10ec-42aa-9b79-8076c738ef0d",
   "metadata": {},
   "source": [
    "### Query Index\n",
    "\n",
    "Execute a query using LMQL + index. Use index to retrieve a set of source documents, and use\n",
    "LMQL to synthesize a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66f51b4-5f68-4ad8-9e9a-03ff5c0a9ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_top_k = 2\n",
    "\n",
    "@lmql.query\n",
    "async def index_query(query_str: str):\n",
    "        '''\n",
    "sample(temperature=0.2, max_len=2048, openai_chunksize=2048)\n",
    "    \"\"\"You are a QA bot that helps users answer questions.\"\"\"\n",
    "    response = index.query(query_str, response_mode=\"no_text\", similarity_top_k=similarity_top_k)\n",
    "    information = \"\\n\\n\".join([s.node.get_text() for s in response.source_nodes])\n",
    "    \"Question: {query_str}\\n\"\n",
    "    \"\\nRelevant Information: {information}\\n\"\n",
    "    \"Your response based on relevant information:[RESPONSE]\"\n",
    "from\n",
    "    \"openai/gpt-3.5-turbo\"\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09504b7-9534-4399-9fb5-689c86386331",
   "metadata": {},
   "outputs": [],
   "source": [
    "await index_query(\"What did the author do growing up?\", output_writer=lmql.stream(variable=\"RESPONSE\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmql_jerry",
   "language": "python",
   "name": "lmql_jerry"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
